검색 항목 -----------------------------------------------------------

DataFrame 특정 행 삭제 방법 / DataFrame 특정 행 또는 열의 Value 접근방법
Gragh Tool(plotly, PyQt5) / range 메서드를 이용한 리스트 만들기

import plotly.express as px
px.bar / px.line 그래프 그리기

Pandsas, Series와 DataFrame / 범주형 데이터 변환 / 비대칭 데이터 문제
데이터 검증, panda DataFrame groupby/sample 함수 / Conv1D, Conv2D, Conv3D

tensorflow 모델 저장 및 불러오기 / Git과 Github의 차이점 & 용어 정리


sklearn module
로지스틱 회귀(regression), 결정트리(Decision tree), 데이터 스케일링(scaler)
K-최근접 이웃(KNeighbors), 배깅(bagging), Boosting Algorithm
AdaBoost, GBM, lightGBM, CatBoost, GridSearchCV
------------------------------------------------------------------


** 주피터 노트북에서 GPU 사용하기 위한 설치 프로그램 **
** 참고 사이트 **
https://theorydb.github.io/dev/2020/02/14/dev-dl-setting-local-python/
https://velog.io/@mactto3487/%EB%94%A5%EB%9F%AC%EB%8B%9D-GPU-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0


** Tip **
pickle : 데이터 압축 시 사용하기도 함



reshape(1, -1) # -1 : 나머지 기준에 맞춘다
막대 그래프에서 색을 리스트로 넣으면 각각 달라짐

from matplotlib import font_manager, rc

font_list = font_manager.findSystemFonts(fontpaths = None, fontext ='ttf')

# Git과 Github의 차이점 & 용어 정리 -----------------------------------------------------

Git : 로컬에서 관리되는 버전 관리 시스템 (VCS : Version Control System)
 + 소스코드 수정에 따른 버전을 관리해주는 시스템
 
Github : 클라우드 방식으로 관리되는 버전 관리 시스템(VCS)
 + 자체 구축이 아닌 빌려쓰는 클라우드 개념
 + 오픈소스는 일정 부분 무료로 저장 가능, 아닐 경우 유료 사용
 
Github
 - Issue : 프로젝트에서 개선되거나 해결되어야할 것을 의미
 - Milestone : 프로젝트에서 주요한 이벤트를 표시하는 기준점이며, 프로젝트의 진척을 관찰하기 위해 사용
	+ 프로젝트가 진행되는 동안 달성되어야 하는 특정한 목표
	+ 예) 주요한 특정 기능의 추가, 버전 업 등이 될 수 있음
	+ 목표 날짜를 설정 가능
	+ 보통 릴리즈를 위해 사용됨

커밋(Commit) : Git(로컬 저장소)에 파일을 추가하거나 변경 내용을 저장하는 작업
푸쉬(Push) : Github(또는 원격 저장소)에 파일을 추가하거나 변경 내용을 저장하는 작업
풀(Pull) : Github(또는 원격 저장소)에서 파일을 다운로드하는 작업
클론(Clone) : Github(또는 원격 저장소)에서 파일을 다운로드(복사)하는 작업

# DataFrame 특정 행 삭제 방법 ---------------------------------------------------------
# '~' 기호 : 삭제의 의미
df = df[~((df['id'] == id) & (df['pw'] == pw)) # 데이터프레임에서 id와 pw가 같은 data를 찾아서 삭제 후 변수에 저장
self.userinfoAll = self.userinfoAll[~( (self.userinfoAll['id'] == id) & (self.userinfoAll['pw'] == pw))]

# DataFrame 특정 행 또는 열의 Value 접근방법 ----------------------------------------------
df['Name'].values[0] # value 값이 int, str 등의 type으로 출력됨 / np.array 타입 등이 아님
print(type(df['Name'].values[0]))

# Gragh Tool
import plotly
import plotly.figure_factory as ff
import plotly.graph_objs as go
import plotly.express as px

# plotly templates 출력 -------------------------------
import plotly.io as pio
print(list(pio.templates))
['ggplot2', 'seaborn', 'simple_white', 'plotly', 'plotly_white', 'plotly_dark', 'presentation'
,'xgridoff', 'ygridoff', 'gridon', 'none']

----------------------------------------------------------------
fig = px.line(df, x='Time', y='Price', title='Yearly Sale Price', template="presentation",
                    line_shape='spline', render_mode="auto")
fig.show()
(x/y : x/y축에 넣은 df 테이블의 column 명)
line_shape : 'linear', 'spline', 'hv', 'vh', 'hvh', 'vhv' (설정 종류)
----------------------------------------------------------------
df = px.data.wind()
fig = px.bar_polar(df, r="frequency", theta="Day",
           color="strength", template="plotly_dark",
           color_discrete_sequence= px.colors.sequential.Plasma_r)
fig.show()
----------------------------------------------------------------
data = px.data.gapminder()
data_canada = data[data.country == 'Canada']
fig = px.bar(data_canada, x='year', y='pop',
             hover_data=['lifeExp', 'gdpPercap'], color='lifeExp',
             labels={'pop':'population of Canada'}, height=400)
fig.show()

# PyQt5 --------------------------------------------------------------------------
from PyQt5.QtWidgets import QWidget, QApplication, QTableWidget, QTableWidgetItem
from PyQt5.QtWidgets import QVBoxLayout, QHeaderView, QSizePolicy

** 출력창 **
def MakeOutputWindow(self, df):
     df_rows = len(df)
     df_columns = len(df.columns.to_list())
     df_cols_names = df.columns.to_list()

     # 모든 Data Type string으로 변환
     df = df.applymap(str)

     app = QApplication(sys.argv)
     qwidget = QWidget()
     qwidget.setWindowTitle("Best Sellers")  # 제목 설정
     qwidget.resize(1676, 1000)  # Window 크기 설정
     # qwidget.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding )

     layout = QVBoxLayout()

     # Table Widget 생성 / 행열 설정
     tableWidget = QTableWidget()
     tableWidget.setColumnCount(df_columns)
     tableWidget.setRowCount(df_rows)

     # Add item in Table
     for i in range(df_rows):
         for x in range(df_columns):
             tableWidget.setItem(i, x, QTableWidgetItem(df.values[i][x]))

     # Set Size-Auto
     tableWidget.horizontalHeader().setSectionResizeMode(QHeaderView.ResizeToContents)

     # Set Column's Labels
     tableWidget.setHorizontalHeaderLabels(df_cols_names)

     layout.addWidget(tableWidget)
     qwidget.setLayout(layout)
     qwidget.show()

     app.exec_()


# range 메서드를 이용한 리스트 만들기 ---------------------------------------------
value = list(range(10)) # 1~9까지 리스트 형태로 만들어짐

# Pandas.DataFrame.corr(method='')  ----------------------------------------
상관계수(correlation coefficient)
: 두 변수가 함께 변하는 정도를 -1 ~ 1 범위의 수로 나타낸 것

method 종류 : pearson(default), kendall, spearman

spearman : 상관분석을 실시함에 있어 서열척도를 사용한 변수가 포함되어 있거나
등간/비율척도를 사용한 변수들이라 하더라도, 두 변수 간의 관계가 비선형적 일 때 구하는 상관계수

kendall : spearman의 상관계수와 마찬가지로, 비선형적 관게이거나 서열변수일 때 사용
spearman의 상관계수보다 믿을만 한 것으로 알려짐 (특히 표본이 작을 때)

# Pandsas, Series와 DataFrame ----------------------------------------------
Pandas Series : Series는 DataFrame의 하위 자료형
1개의 열(column)이 Serie, 이 Serie가 다수 모여 데이터프레임을 형성

Pandas DataFrame
Dictionary -> DataFrame : Value가 List 형태여야 함
안좋은 예시) dict_data = {'a':1,'b':2,'c':3}
좋은 예시) dict_data = {'a':[1],'b':[2],'c':[3]}

# 범주형 데이터 변환 ----------------------------------------------------------
방법 1.
df['column명'] = df['column명'].apply(lambda x: x.astype('category').cat.codes)
** 설명 ** 
apply() : 특정 column을 일괄적으로 변경할 때 사용
Ex) df['column명'] = df['column명'].apply(함수명)
astype('category') 강제 형변환
(def =)lambda : column의 값에 내가 정해놓은 함수 return값을 일괄적으로 적용
Ex) apply(lambda 입력값 : 리턴값(결과값))
cat.categories : 카테고리 값 / cat.codes 자동으로 숫자형 리턴

방법 2.
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
res = le.fit_transform(data['column명'])

# 역변환
le.inverse_transform(res)

# column 생성 및 value 입력
df['column생성명'] = res

방법 3.
# 원핫인코딩
df['column명'] = data['column명'].astype('category') # str -> category
# df['column명'] = df['column명'].astype('str') # category -> str 

&& 차이점 확인해보기
pd.get_dummies(df, columns=['column명']) # category type으로 변환 꼭 필요
pd.get_dummies(df['column명'])

# 비대칭 데이터 문제 ----------------------------------------------------------
1. Over-Sampling(오버) : 소수 클래스 데이터를 증가
	- from imblearn.over_sampling import SMOTE
2. Under-Sampling(언더) : 다수 클래스 데이터에서 일부만 사용
	- from imblearn.under_sampling import (찾아봐야함)
3. Combining Over- and Under-Sampling(복합)
** 참고 사이트 ** 
https://datascienceschool.net/03%20machine%20learning/14.02%20%EB%B9%84%EB%8C%80%EC%B9%AD%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%AC%B8%EC%A0%9C.html

# 데이터 스케일링 ----------------------------------------------------------
StandardScaler : 각 특성의 평균을 0, 분산을 1로 변경하여 특성의 스케일을 맞춤
RobustScaler : 평균과 분산 대신에 중간 값과 사분위 값을 사용
MinMaxScaler : 모든 특성이 0과 1 사이에 위치하도록 데이터를 변경
Normalizer : 특성 벡터의 유클리디안 길이가 1이되도록 조정 ( 아래 참고 사이트에서 그림보는 것을 추천 )
 - 길이가 1인 원 또는 구로 투영하는 것이고, 각도만이 중요할 때 적용

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

스케일된 X_train을 이후
model.fit(X_train_scaled, y_train).score(X_test, y_test)
fit X값에 넣어 학습시킴

** 참고 사이트 **
https://subinium.github.io/MLwithPython-3-3/

# 로지스틱 회귀(모델링) ----------------------------------------------------------
특징 : Sample이 특정 클래스에 속할 확률을 추정하는데 주로 사용
예) 이메일이 스팸일 확률 등..
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(X_train,y_train).score(X_test, y_test)
fit : 학습시키겠다. / score : 정확도 계산 (pu법)

# 결정트리(모델링) ----------------------------------------------------------
정의 :  if else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘
	or 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 알고리즘
	
장점 : 쉽고 직관적 / 각 피처의 스케일링과 정규화 같은 전처리 작업의 영향도가 작음
단점 : 규칙을 추가하며 서브트리를 만들어 나갈수록 모델이 복잡 -> 과적합에 빠지기 쉬움
	-> 튜닝 필요

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier()

tree.fit(X_train,y_train).score(X_test, y_test)

** 참고 사이트 ** 
https://injo.tistory.com/15#Decision-Tree%EC%9D%98-%EA%B5%AC%EC%A1%B0

# K-최근접 이웃(모델링) ----------------------------------------------------------
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

knn.fit(X_train,y_train).score(X_test, y_test)

** 참고 사이트 ** 
http://hleecaster.com/ml-knn-concept/

# 다수결 분류(모델링) ----------------------------------------------------------
1. Hard Voting Classifier
: 여러 모델을 생성하고 그 성과(결과)를 비교함, 이 때 classifier의 결과들을 집계하여 
	가장 많은 표를 얻는 클래스를 최종 예측값으로 정함
2. Soft Voting Classifier
:앙상블에 사용되는 모든 분류기가 클래스의 확률을 예측할 수 있을 때 사용
	각 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측값으로 정함
from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(
    estimators = [('logreg', logreg), ('tree', tree), ('knn', knn)], # 3가지
    voting = 'hard')

voting.fit(X_train,y_train).score(X_test, y_test)

** 참고 사이트 **
https://nonmeyet.tistory.com/entry/Python-Voting-Classifiers%EB%8B%A4%EC%88%98%EA%B2%B0-%EB%B6%84%EB%A5%98%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EA%B5%AC%ED%98%84

# 배깅(모델링) ----------------------------------------------------------
배깅(bagging, bootstrap agrregating) : 훈련세트에서 중복을 허용하여 샘플링하는 방식
페이스팅(pasting) : 훈련세트에서 중복을 허용하지 않고 샘플링하는 방식

중복을 허용한 랜덤 샘플링으로 만든 훈련 세트(부트스트랩)를 사용하여 각 내부 모델을 다르게 학습
(훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습)

from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(), random_state=0, n_estimators=200)
bagging.fit(X_train, y_train).score(X_test, y_test)

** 참고 사이트 **
https://yganalyst.github.io/ml/ML_chap6-2/

# Boosting Algorithm ------------------------------------------------------
부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 
데이터에 가중치를 부여해 오류를 개선해나가며 학습하는 방식

** 종류 **
AdaBoost, LPBoost, TotalBoost, BrownBoost, MadaBoost, 
LogitBoost, Gradient Boosting Machine(GBM), lightGBM 등 많은 종류가 존재

** 대표적인 종류 **
AdaBoost, Gradient Boosting Machine(GBM), lightGBM, XGBoost, CatBoost

** 차이점 ** 
Boosting 기법들의 차이는 오분류된 데이터를 다음 라운드에서 어떻게 반영시킬건지의 차이

** 참고 사이트 ** 
https://injo.tistory.com/31

# AdaBoost(모델링) ----------------------------------------------------------
from sklearn.ensemble import AdaBoostClassifier # 많이 사용

ada = AdaBoostClassifier(n_estimators=200, random_state=0)
ada.fit(X_train, y_train).score(X_test, y_test)

# GBM(모델링) ----------------------------------------------------------
from sklearn.ensemble import GradientBoostingClassifier

gbm = GradientBoostingClassifier(n_estimators=200, random_state=0)
gbm.fit(X_train, y_train).score(X_test, y_test)

# lightGBM(모델링) ----------------------------------------------------------
from lightgbm import LGBMClassifier # 다른 알고리즘에 비해 최근에 나옴, 비교적 성능이 좋음

lgb = LGBMClassifier(n_estimaotrs = 200, random_state=0) # 파라미터 설정에 따라 성능이 많이 달라짐
lgb.fit(X_train, y_train).score(X_test, y_test)


# 데이터 검증 ----------------------------------------------------------
from sklearn.model_selection import cross_val_score # valudation 확인
from sklearn.svm import SVC

model = SVC() # 단순하게 빠름, 많이 쓰임
scores = cross_val_score(model, X_train, y_train); scores

** + 상세 옵션 ** 
from sklearn.model_selection import cross_val_score # valudation 확인
from sklearn.svm import SVC
# 교차 검증 상세 옵션
from sklearn.model_selection import KFold

model = SVC() # 단순하게 빠름, 많이 쓰임

kfold = KFold(n_splits=5, shuffle=True, random_state=0) # KFold 객체 생성
scores = cross_val_score(model, X_train, y_train, cv=kfold)

** 참고 사이트 **
https://woolulu.tistory.com/70?category=780979
    + 다양한 교차 검증 방법 또한 해당 참고 사이트의 다른 페이지에 기재 되어있음

# panda DataFrame groupby 함수 -----------------------------------------
groupby() 연산자를 사용하여 집단, 그룹별로 데이터를 집계, 요약하는 방법

전체 데이터를 그룹 별로 나누고 (split), 각 그룹별로 집계함수를 적용(apply) 후, 
그룹별 집계 결과를 하나로 합치는(combine) 단계를 거치게 됨
[ Split -> Apply -> Combine ]

집단별 크기 : grouped.size()
집단별 합계 : grouped.sum()
집단별 평균 : grouped.mean()

** 참고 사이트 ** 
https://rfriend.tistory.com/383 (이미지 참고하기)

# panda DataFrame sample 함수 --------------------------------------------------
DataFrame에서 랜덤으로 데이터를 뽑을 때 사용

1. df.sample(n=5) : df에서 5개의 row를 랜덤으로 뽑음
2. df.sample(frac=0.7) : n 대신 frac을 입력하면 전체 Row 데이터에서 70%의 데이터를 뽑음
3. df.sample(frac=0.7).reset_index(drop=True)도 가능 : drop은 인덱스 열을 지움


# CatBoost(모델링) --------------------------------------------------
CatBoost : Categorical Boost

Cardinality(카디널리티)는 전체 행에 대한 특정 컬럼의 중복 수치를 나타내는 지표

중복도가 ‘낮으면’ 카디널리티가 ‘높다’ => 고유값이 많다
중복도가 ‘높으면’ 카디널리티가 ‘낮다’ => 고유값이 적다

** 참고 사이트 **
https://dailyheumsi.tistory.com/136

# GridSearchCV 모듈 ---------------------------------------------------
사이킷런에서는 분류 알고리즘이나 회귀 알고리즘에 사용되는 하이퍼파라미터를 순차적으로 
입력해 학습을 하고 측정을 하면서 가장 좋은 파라미터를 알려줌

from sklearn.model_selection import GridSearchCV 살펴보기
하이퍼 파라미터의 최적값을 찾아줌
모든 경우의 수를 쓰기 때문에 시간이 많이 걸림

** 참고 사이트 **
https://teddylee777.github.io/scikit-learn/grid-search-%EB%A1%9C-hyperparameter%EC%B5%9C%EC%A0%81%ED%99%94

머신러닝에서 문제점
ordering principle

생성파지콘볼루션신경망(GGCNN) 알아보기


딥러닝 기반 객체인식 로봇 팔 제어 시스템
https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201831342441269&oCn=NPAP12689376&dbt=CFKO&journal=NPRO00377596

# Conv1D, Conv2D, Conv3D ---------------------------------------------------
Dimension : 공간적인 측면을 기준으로 1D / 2D / 3D로 나뉨

Conv1D : [1 Dimension] 자연어 처리 분야에서 주로 사용
Conv2D : [2 Dimension] 이미지 처리 분야에서 주로 사용
Conv3D : [3 Dimension] ??


# tensorflow 모델 저장 및 불러오기 ---------------------------------------------------
** 참고 사이트 **
https://minimin2.tistory.com/105

# from sklearn.feature_selection import SelectFromModel ------------------------
select = SelectFromModel(RandomForestClassifier(), threshold=None)

X_train_fs = select.fit(X_train, y_train).trainform(X_train)

mask = select.get_support()

# 주피터 노트북 chapter15_MachineLearning3(pipeline).ipynb 파일 보기